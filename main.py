#!/usr/bin/env python3
"""
çŸ¥è¯†åº“ç®¡ç†MCPæœåŠ¡å™¨

åŸºäºFastMCPæ„å»ºçš„çŸ¥è¯†åº“ç®¡ç†å·¥å…·ï¼Œæ”¯æŒç›®å½•æ ‘æŸ¥çœ‹ã€å†…å®¹æœç´¢å’ŒCollectionæŸ¥çœ‹åŠŸèƒ½ã€‚
é‡æ„ç‰ˆæœ¬ï¼Œé‡‡ç”¨æ›´å¥½çš„æ¶æ„è®¾è®¡å’Œç»Ÿä¸€çš„APIå®¢æˆ·ç«¯ã€‚
"""

import os
import asyncio
from typing import List
from fastmcp import FastMCP

# å¯¼å…¥æ–°çš„æ¶æ„ç»„ä»¶
from src.config import config
from src.services import TreeService, SearchService, CollectionService, DocumentAnalysisService, FormatUtils

# æ³¨æ„ï¼šä¼ ç»Ÿå·¥å…·ä¾èµ–å·²ç§»é™¤ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨

# åŠ è½½æ—¥å¿—é…ç½®
try:
    from dotenv import load_dotenv
    load_dotenv("config/logging.env")
except:
    pass  # å¦‚æœæ—¥å¿—é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®

# åˆ›å»ºFastMCPå®ä¾‹
mcp = FastMCP("çŸ¥è¯†åº“ç®¡ç†å·¥å…· v2.0")

# åˆ›å»ºæœåŠ¡å®ä¾‹
tree_service = TreeService()
search_service = SearchService()
collection_service = CollectionService()
document_analysis_service = DocumentAnalysisService()


@mcp.tool("get_dataset_tree")
async def get_kb_tree(search_value: str = "", deep: int = 4) -> str:
    """
    ğŸ“ è·å–çŸ¥è¯†åº“ç›®å½•æ ‘
    
    æµè§ˆçŸ¥è¯†åº“çš„ç›®å½•ç»“æ„ï¼ŒæŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†å’Œæ–‡ä»¶å¤¹ã€‚
    ç”¨äºäº†è§£çŸ¥è¯†åº“çš„ç»„ç»‡æ¶æ„ï¼Œæ‰¾åˆ°ç›¸å…³çš„æ•°æ®é›†IDç”¨äºåç»­æœç´¢ã€‚
    
    å‚æ•°:
        - search_value: è¿‡æ»¤å…³é”®è¯ï¼ˆå¯é€‰ï¼‰ï¼Œæ”¯æŒå¤šå…³é”®è¯ç©ºæ ¼åˆ†éš”ï¼Œå¦‚"äºšä¿¡ IPOSS"æˆ–"ç½‘ç»œç®¡ç† ç³»ç»Ÿ"
        - deep: ç›®å½•æ·±åº¦ï¼ˆ1-10ï¼Œé»˜è®¤4ï¼‰
    
    è¿”å›: åŒ…å«æ•°æ®é›†IDã€åç§°ã€ç±»å‹çš„ç›®å½•æ ‘ç»“æ„
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯: åœ¨è¿›è¡Œæ–‡æ¡£æœç´¢å‰ï¼Œå…ˆäº†è§£æœ‰å“ªäº›å¯ç”¨çš„æ•°æ®é›†
    ğŸ” æœç´¢å¢å¼º: æ”¯æŒå¤šå…³é”®è¯å¹¶å‘æœç´¢ï¼Œè‡ªåŠ¨å»é‡åˆå¹¶ç»“æœ
    """
    parent_id = config.get_parent_id()
    return await tree_service.get_knowledge_base_tree(parent_id, search_value, deep)


@mcp.tool("search_dataset")
async def search_kb(dataset_id: str, text: str, limit: int = 10) -> str:
    """
    ğŸ” å•æ•°æ®é›†ç²¾ç¡®æœç´¢
    
    åœ¨æŒ‡å®šçš„å•ä¸ªæ•°æ®é›†ä¸­æœç´¢ç›¸å…³å†…å®¹ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚
    é€‚ç”¨äºå·²çŸ¥ç›®æ ‡æ•°æ®é›†çš„ç²¾ç¡®æœç´¢ï¼Œæ‰¾åˆ°ç‰¹å®šæ–‡æ¡£çš„ç›¸å…³ç‰‡æ®µã€‚
    
    å‚æ•°:
        - dataset_id: æ•°æ®é›†IDï¼ˆé€šè¿‡get_dataset_treeè·å–ï¼‰
        - text: æœç´¢å…³é”®è¯
        - limit: ç»“æœæ•°é‡ï¼ˆ1-50ï¼Œé»˜è®¤10ï¼‰
    
    è¿”å›: åŒ…å«æ–‡æ¡£ç‰‡æ®µã€ç›¸å…³æ€§è¯„åˆ†å’Œæ¥æºä¿¡æ¯çš„æœç´¢ç»“æœ
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯: å®šä½ç‰¹å®šæ•°æ®é›†ä¸­çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
    """
    return await search_service.search_knowledge_base(dataset_id, text, limit)


@mcp.tool("view_collection_content")
async def view_collection_content_tool(collection_id: str, page_size: int = 50) -> str:
    """
    ğŸ“„ æŸ¥çœ‹æ–‡æ¡£å®Œæ•´å†…å®¹
    
    è·å–æŒ‡å®šæ–‡æ¡£ï¼ˆcollectionï¼‰çš„æ‰€æœ‰å†…å®¹å—ï¼ŒæŸ¥çœ‹å®Œæ•´çš„æ–‡æ¡£å†…å®¹ã€‚
    é€‚ç”¨äºæŸ¥çœ‹æœç´¢åˆ°çš„æ–‡æ¡£çš„å®Œæ•´ä¿¡æ¯ã€‚
    
    å‚æ•°:
        - collection_id: æ–‡æ¡£IDï¼ˆä»æœç´¢ç»“æœä¸­è·å–ï¼‰
        - page_size: æ¯é¡µæ•°æ®å—æ•°é‡ï¼ˆ10-100ï¼Œé»˜è®¤50ï¼‰
    
    è¿”å›: åŒ…å«å®Œæ•´æ–‡æ¡£å†…å®¹ã€æ–‡ä»¶ä¿¡æ¯å’Œä¸‹è½½é“¾æ¥çš„è¯¦ç»†æŠ¥å‘Š
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯: æŸ¥çœ‹æœç´¢å®šä½åˆ°çš„æ–‡æ¡£çš„å®Œæ•´å†…å®¹
    """
    return await collection_service.view_collection_content(collection_id, page_size)


@mcp.tool("intelligent_search_and_answer")
async def intelligent_search_and_answer(question: str, available_datasets: List[str], generate_answer: bool = True) -> str:
    """
    ğŸ¤– ä¼ ç»Ÿæ™ºèƒ½æœç´¢é—®ç­”
    
    åŸºäºå…³é”®è¯å’Œæœç´¢è®¡åˆ’çš„ä¼ ç»Ÿæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚
    ä¸»è¦åŸºäºæœç´¢ç‰‡æ®µè¿›è¡Œåˆ†æï¼Œä¸è·å–å®Œæ•´æ–‡æ¡£å†…å®¹ã€‚
    
    å‚æ•°:
        - question: ç”¨æˆ·é—®é¢˜
        - available_datasets: å¯ç”¨æ•°æ®é›†IDåˆ—è¡¨  
        - generate_answer: æ˜¯å¦ç”Ÿæˆç»¼åˆç­”æ¡ˆï¼ˆé»˜è®¤Trueï¼‰
    
    è¿”å›: åŸºäºæœç´¢ç‰‡æ®µçš„åˆ†æç»“æœå’Œç­”æ¡ˆ
    
    ğŸ’¡ å»ºè®®: ä¼˜å…ˆä½¿ç”¨ smart_document_analysis è·å¾—æ›´å‡†ç¡®çš„å…¨æ–‡åˆ†æ
    """
    # ä½¿ç”¨æ–°çš„æ™ºèƒ½æ–‡æ¡£åˆ†ææœåŠ¡ä½œä¸ºæ›¿ä»£å®ç°
    if not generate_answer:
        # å¦‚æœä¸ç”Ÿæˆç­”æ¡ˆï¼Œåˆ™è¿›è¡Œå¤šæ•°æ®é›†æœç´¢
        return await multi_dataset_search(available_datasets, question, limit_per_dataset=10)
    
    # é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨æ™ºèƒ½æ–‡æ¡£åˆ†æ
    result = await document_analysis_service.analyze_documents_for_question(
        question,
        available_datasets,
        max_docs=5,
        max_search_results=20
    )
    
    # æ·»åŠ è¯´æ˜ï¼Œå‘ŠçŸ¥ç”¨æˆ·ä½¿ç”¨äº†å‡çº§ç‰ˆåŠŸèƒ½
    enhanced_result = f"""# ğŸ”„ æ™ºèƒ½æœç´¢é—®ç­”ç»“æœ
> **æ³¨æ„**: ä¼ ç»ŸåŠŸèƒ½å·²å‡çº§ä¸ºæ›´å¼ºå¤§çš„æ™ºèƒ½æ–‡æ¡£åˆ†æï¼Œæä¾›å®Œæ•´æ–‡æ¡£å†…å®¹åˆ†æ

{result}

---
ğŸ’¡ **åŠŸèƒ½å‡çº§è¯´æ˜**: æœ¬æ¬¡æœç´¢ä½¿ç”¨äº†å‡çº§ç‰ˆçš„æ–‡æ¡£åˆ†æåŠŸèƒ½ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ç‰‡æ®µæœç´¢ï¼Œèƒ½å¤Ÿï¼š
- ğŸ“„ è·å–å®Œæ•´æ–‡æ¡£å†…å®¹ï¼ˆè€Œéä»…ç‰‡æ®µï¼‰
- ğŸ¯ æä¾›æ›´å‡†ç¡®çš„ç»¼åˆåˆ†æ
- ğŸ“Š è¯¦ç»†çš„æœç´¢å’Œåˆ†æè¿‡ç¨‹æŠ¥å‘Š
"""
    
    return enhanced_result


@mcp.tool("generate_search_plan")
async def generate_search_plan_tool(question: str, available_datasets: List[str]) -> str:
    """
    ğŸ“‹ ç”Ÿæˆæœç´¢è®¡åˆ’
    
    åˆ†æç”¨æˆ·é—®é¢˜å¹¶ç”Ÿæˆè¯¦ç»†çš„æœç´¢ç­–ç•¥å’Œä»»åŠ¡åˆ—è¡¨ã€‚
    ä¸»è¦ç”¨äºäº†è§£æœç´¢ç­–ç•¥ï¼Œä¸æ‰§è¡Œå®é™…æœç´¢ã€‚
    
    å‚æ•°:
        - question: ç”¨æˆ·é—®é¢˜
        - available_datasets: å¯ç”¨æ•°æ®é›†IDåˆ—è¡¨
    
    è¿”å›: è¯¦ç»†çš„æœç´¢è®¡åˆ’å’Œç­–ç•¥
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯: äº†è§£æœç´¢ç­–ç•¥ï¼Œé…åˆå…¶ä»–å·¥å…·ä½¿ç”¨
    """
    # åˆ†æé—®é¢˜å¹¶ç”Ÿæˆæœç´¢è®¡åˆ’
    import re
    
    # æå–å…³é”®è¯
    question_clean = re.sub(r'[^\w\s]', ' ', question)
    keywords = [word for word in question_clean.split() if len(word) > 1]
    
    # ç”Ÿæˆæœç´¢è®¡åˆ’
    plan = f"""# ğŸ“‹ æ™ºèƒ½æœç´¢è®¡åˆ’

## ğŸ¯ é—®é¢˜åˆ†æ
**åŸé—®é¢˜**: {question}

**æå–çš„å…³é”®è¯**: {', '.join(keywords)}

## ğŸ“Š æ•°æ®é›†ä¿¡æ¯
**å¯ç”¨æ•°æ®é›†æ•°é‡**: {len(available_datasets)}
**æ•°æ®é›†åˆ—è¡¨**:
"""
    
    for i, dataset_id in enumerate(available_datasets, 1):
        plan += f"  {i}. `{dataset_id[:12]}...`\n"
    
    plan += f"""

## ğŸ” æœç´¢ç­–ç•¥

### 1. å¤šå…³é”®è¯æœç´¢ç­–ç•¥
- **ä¸»è¦å…³é”®è¯**: {keywords[:3] if len(keywords) >= 3 else keywords}
- **æ¬¡è¦å…³é”®è¯**: {keywords[3:] if len(keywords) > 3 else 'æ— '}

### 2. æ•°æ®é›†æœç´¢ä¼˜å…ˆçº§
- **ä¼˜å…ˆçº§**: æŒ‰æ•°æ®é›†IDé¡ºåºæœç´¢
- **æœç´¢æ·±åº¦**: æ¯ä¸ªæ•°æ®é›†è¿”å›20ä¸ªç»“æœ
- **ç»“æœç­›é€‰**: åŸºäºç›¸å…³æ€§è¯„åˆ†æ’åº

### 3. ç»“æœåˆå¹¶ç­–ç•¥
- **å»é‡æœºåˆ¶**: åŸºäºæ–‡æ¡£IDå»é‡
- **æ’åºæ–¹å¼**: æŒ‰ç›¸å…³æ€§è¯„åˆ†é™åº
- **æœ€å¤§ç»“æœæ•°**: 5ä¸ªæœ€ç›¸å…³æ–‡æ¡£

## ğŸ› ï¸ æ‰§è¡Œæ­¥éª¤

### æ­¥éª¤1: å¤šæ•°æ®é›†æœç´¢
```
multi_dataset_search(
    dataset_ids={available_datasets},
    query="{question}",
    limit_per_dataset=20
)
```

### æ­¥éª¤2: è·å–å®Œæ•´æ–‡æ¡£å†…å®¹
åŸºäºæœç´¢ç»“æœè·å–æœ€ç›¸å…³çš„5ä¸ªæ–‡æ¡£çš„å®Œæ•´å†…å®¹

### æ­¥éª¤3: æ™ºèƒ½åˆ†æ
ä½¿ç”¨AIå¯¹å®Œæ•´æ–‡æ¡£å†…å®¹è¿›è¡Œåˆ†æï¼Œç”Ÿæˆç»¼åˆç­”æ¡ˆ

## ğŸ’¡ æ¨èæ‰§è¡Œæ–¹æ¡ˆ

**æ–¹æ¡ˆA: ä¸€é”®æ™ºèƒ½åˆ†æï¼ˆæ¨èï¼‰**
```
smart_document_analysis(
    question="{question}",
    dataset_ids={available_datasets},
    max_docs=5,
    max_search_results=20
)
```

**æ–¹æ¡ˆB: åˆ†æ­¥æ‰§è¡Œ**
1. å…ˆæ‰§è¡Œ `multi_dataset_search` äº†è§£æœç´¢ç»“æœåˆ†å¸ƒ
2. å†ä½¿ç”¨ `view_collection_content` æŸ¥çœ‹å…·ä½“æ–‡æ¡£
3. æœ€åæ‰‹åŠ¨åˆ†ææ•´åˆä¿¡æ¯

---
âš¡ **å»ºè®®**: ç›´æ¥ä½¿ç”¨æ–¹æ¡ˆAè¿›è¡Œä¸€é”®æ™ºèƒ½åˆ†æï¼Œæ•ˆç‡æ›´é«˜ä¸”ç»“æœæ›´å‡†ç¡®ã€‚
"""
    
    return plan


@mcp.tool("smart_document_analysis")
async def smart_document_analysis(question: str, dataset_ids: List[str], max_docs: int = 5, max_search_results: int = 20) -> str:
    """
    ğŸ§  æ™ºèƒ½æ–‡æ¡£åˆ†æ
    
    **æ¨èä½¿ç”¨çš„æ ¸å¿ƒå·¥å…·** - å®ç°å®Œæ•´çš„æ™ºèƒ½æ–‡æ¡£åˆ†æå·¥ä½œæµï¼š
    1. ğŸ” å¤šæ•°æ®é›†æœç´¢å®šä½ç›¸å…³æ–‡æ¡£
    2. ğŸ“„ è·å–å®šä½æ–‡æ¡£çš„å®Œæ•´å†…å®¹
    3. ğŸ¯ åˆ†æå¤šä¸ªæ–‡æ¡£å†…å®¹
    4. âœ¨ ç”Ÿæˆç»¼åˆæ€§ç­”æ¡ˆ
    
    å‚æ•°:
        - question: è¦åˆ†æçš„é—®é¢˜
        - dataset_ids: è¦æœç´¢çš„æ•°æ®é›†IDåˆ—è¡¨
        - max_docs: æœ€å¤§åˆ†ææ–‡æ¡£æ•°é‡ï¼ˆ1-10ï¼Œé»˜è®¤5ï¼‰
        - max_search_results: æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æœç´¢ç»“æœæ•°ï¼ˆ5-50ï¼Œé»˜è®¤20ï¼‰
    
    è¿”å›: åŒ…å«ç»¼åˆç­”æ¡ˆã€ç›¸å…³æ–‡æ¡£å†…å®¹å’Œæœç´¢è¿‡ç¨‹çš„å®Œæ•´åˆ†ææŠ¥å‘Š
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯: åŸºäºé—®é¢˜æ™ºèƒ½åˆ†æå¤šä¸ªæ–‡æ¡£å¹¶ç”Ÿæˆç»¼åˆç­”æ¡ˆçš„é¦–é€‰å·¥å…·
    """
    return await document_analysis_service.analyze_documents_for_question(
        question,
        dataset_ids,
        max_docs,
        max_search_results
    )


@mcp.tool("multi_dataset_search")
async def multi_dataset_search(dataset_ids: List[str], query: str, limit_per_dataset: int = 5) -> str:
    """
    ğŸ” å¤šæ•°æ®é›†æœç´¢
    
    åœ¨å¤šä¸ªæ•°æ®é›†ä¸­åŒæ—¶æœç´¢ç›¸åŒæŸ¥è¯¢ï¼Œå¿«é€Ÿå®šä½ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‚
    é€‚ç”¨äºè·¨å¤šä¸ªæ•°æ®é›†çš„åˆæ­¥æœç´¢ã€‚
    
    å‚æ•°:
        - dataset_ids: æ•°æ®é›†IDåˆ—è¡¨
        - query: æœç´¢å…³é”®è¯
        - limit_per_dataset: æ¯ä¸ªæ•°æ®é›†çš„ç»“æœé™åˆ¶ï¼ˆé»˜è®¤5ï¼‰
    
    è¿”å›: å„æ•°æ®é›†çš„æœç´¢ç»“æœæ±‡æ€»
    
    ğŸ’¡ ä½¿ç”¨åœºæ™¯: å¿«é€Ÿäº†è§£å¤šä¸ªæ•°æ®é›†ä¸­çš„ç›¸å…³å†…å®¹åˆ†å¸ƒ
    """
    # ä½¿ç”¨æ–°æ¶æ„çš„æœç´¢æœåŠ¡
    try:
        results_by_dataset = {}
        total_results = 0
        
        # åœ¨æ¯ä¸ªæ•°æ®é›†ä¸­æœç´¢
        for dataset_id in dataset_ids:
            try:
                # ä½¿ç”¨æ–°æ¶æ„çš„æœç´¢æœåŠ¡
                dataset_results = await search_service.search_knowledge_base_raw(
                    dataset_id, 
                    query, 
                    limit_per_dataset
                )
                results_by_dataset[dataset_id] = dataset_results
                total_results += len(dataset_results)
            except Exception as e:
                results_by_dataset[dataset_id] = []
                
        # ä½¿ç”¨ç»Ÿä¸€æ ¼å¼åŒ–å·¥å…·ç”Ÿæˆå¤´éƒ¨
        markdown = FormatUtils.format_multi_search_summary(len(dataset_ids), total_results, query)
        
        for dataset_id, results in results_by_dataset.items():
            markdown += f"""### æ•°æ®é›†: {dataset_id[:8]}...
**ç»“æœæ•°é‡**: {len(results)}

"""
            if results:
                for i, result in enumerate(results[:3], 1):  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                    score = sum(s.get("value", 0) for s in result.score) if result.score else 0
                    
                    # è·å–æ–‡ä»¶ä¸‹è½½é“¾æ¥å’Œè¯¦ç»†ä¿¡æ¯
                    try:
                        download_link = await search_service.api_client.get_file_download_link(result.collection_id)
                        collection_detail = await search_service.api_client.get_collection_detail(result.collection_id)
                    except:
                        download_link = None
                        collection_detail = None
                    
                    markdown += f"""#### ç»“æœ {i}
**å†…å®¹**: {result.q[:200]}{'...' if len(result.q) > 200 else ''}

**ç›¸å…³æ€§è¯„åˆ†**: {score:.4f}

"""
                    
                    # ä½¿ç”¨ç»Ÿä¸€æ ¼å¼åŒ–å·¥å…·ç”Ÿæˆæ¥æºä¿¡æ¯
                    source_info = FormatUtils.format_source_info_block(
                        collection_id=result.collection_id,
                        source_name=result.source_name,
                        download_link=download_link,
                        collection_detail=collection_detail
                    )
                    markdown += source_info
                    
                    markdown += f"""ğŸ’¡ *å¯ä½¿ç”¨Collection IDæŸ¥çœ‹å®Œæ•´æ–‡æ¡£: `view_collection_content(collection_id="{result.collection_id}")`*

---

"""
            else:
                markdown += "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ\n\n"
        
        return markdown
        
    except Exception as e:
        return f"# å¤šæ•°æ®é›†æœç´¢é”™è¯¯\n\n{str(e)}"


def main():
    """ä¸»å‡½æ•°"""
    from src.logger import server_logger
    
    server_logger.info("ğŸš€ å¯åŠ¨çŸ¥è¯†åº“ç®¡ç†MCPæœåŠ¡å™¨ v2.0")
    server_logger.info(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    server_logger.info(f"ğŸ”‘ é…ç½®çš„çˆ¶çº§ç›®å½•ID: {config.get_parent_id()[:8]}...")
    
    # æ˜¾ç¤ºå¯ç”¨çš„å·¥å…·
    server_logger.info("ğŸ› ï¸  å·²æ³¨å†Œçš„MCPå·¥å…·:")
    server_logger.info("  ğŸ“ get_dataset_tree - è·å–çŸ¥è¯†åº“ç›®å½•æ ‘")
    server_logger.info("  ğŸ” search_dataset - å•æ•°æ®é›†ç²¾ç¡®æœç´¢") 
    server_logger.info("  ğŸ“„ view_collection_content - æŸ¥çœ‹æ–‡æ¡£å®Œæ•´å†…å®¹")
    server_logger.info("  ğŸ§  smart_document_analysis - ã€æ¨èã€‘æ™ºèƒ½æ–‡æ¡£åˆ†æï¼ˆæœç´¢â†’å…¨æ–‡â†’ç­”æ¡ˆï¼‰")
    server_logger.info("  ğŸ” multi_dataset_search - å¤šæ•°æ®é›†å¿«é€Ÿæœç´¢")
    server_logger.info("  ğŸ¤– intelligent_search_and_answer - ä¼ ç»Ÿæ™ºèƒ½æœç´¢é—®ç­”")
    server_logger.info("  ğŸ“‹ generate_search_plan - ç”Ÿæˆæœç´¢è®¡åˆ’")
    
    server_logger.info(f"ğŸŒ å¯åŠ¨SSEæœåŠ¡å™¨: http://{config.mcp_host}:{config.mcp_port}")
    server_logger.info(f"ğŸ”— SSEç«¯ç‚¹: http://{config.mcp_host}:{config.mcp_port}/sse")
    server_logger.info("âš™ï¸  MCPå®¢æˆ·ç«¯é…ç½®:")
    server_logger.info(f'  "url": "http://{config.mcp_host}:{config.mcp_port}/sse?parentId={config.get_parent_id()}"')
    server_logger.info("=" * 60)
    
    # å¯åŠ¨MCPæœåŠ¡å™¨
    mcp.run(transport="sse", host=config.mcp_host, port=config.mcp_port)


if __name__ == "__main__":
    main() 