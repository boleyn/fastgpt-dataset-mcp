#!/usr/bin/env python3
"""
çŸ¥è¯†åº“ç®¡ç†MCPæœåŠ¡å™¨

åŸºäºFastMCPæ„å»ºçš„çŸ¥è¯†åº“ç®¡ç†å·¥å…·ï¼Œæ”¯æŒç›®å½•æ ‘æŸ¥çœ‹ã€å†…å®¹æœç´¢å’ŒCollectionæŸ¥çœ‹åŠŸèƒ½ã€‚
é‡æ„ç‰ˆæœ¬ï¼Œé‡‡ç”¨æ›´å¥½çš„æ¶æ„è®¾è®¡å’Œç»Ÿä¸€çš„APIå®¢æˆ·ç«¯ã€‚
"""

import os
import asyncio
from typing import List
from fastmcp import FastMCP, Context

# å¯¼å…¥æ–°çš„æ¶æ„ç»„ä»¶
from src.config import config
from src.services import TreeService, SearchService, CollectionService, FormatUtils

# ä¼šè¯çº§åˆ«çš„parentIdå­˜å‚¨ - ä½¿ç”¨session_idä½œä¸ºkey
SESSION_PARENT_IDS = {}

def get_parent_id_from_context(ctx: Context) -> str:
    """ä»Contextä¸­è·å–æˆ–è®¾ç½®parentId"""
    from src.logger import server_logger
    
    # è·å–ä¼šè¯æ ‡è¯†
    session_id = getattr(ctx, 'client_id', None) or getattr(ctx, 'request_id', None) or 'default'
    
    # é¦–å…ˆå°è¯•ä»HTTPè¯·æ±‚ä¸­æå–parentIdï¼ˆSSEè¿æ¥æ—¶ï¼‰
    current_url_parent_id = None
    try:
        # ä½¿ç”¨æ–°çš„ä¾èµ–å‡½æ•°è·å–HTTPè¯·æ±‚
        from fastmcp.server.dependencies import get_http_request
        request = get_http_request()
        
        if request and hasattr(request, 'query_params'):
            query_params = request.query_params
            if 'parentId' in query_params:
                parent_id = query_params['parentId']
                if parent_id and parent_id.strip():
                    current_url_parent_id = parent_id.strip()
    except Exception as e:
        server_logger.debug(f"æ— æ³•è·å–HTTPè¯·æ±‚æˆ–æå–URLå‚æ•°: {e}")
    
    # å¦‚æœURLä¸­æœ‰parentIdï¼Œæ£€æŸ¥æ˜¯å¦ä¸ä¼šè¯ä¸­å­˜å‚¨çš„ä¸åŒ
    if current_url_parent_id:
        stored_parent_id = SESSION_PARENT_IDS.get(session_id)
        
        if stored_parent_id != current_url_parent_id:
            # URLä¸­çš„parentIdä¸å­˜å‚¨çš„ä¸åŒï¼Œæ›´æ–°å­˜å‚¨
            SESSION_PARENT_IDS[session_id] = current_url_parent_id
            server_logger.info(f"ğŸ”„ æ£€æµ‹åˆ°parentIdå˜åŒ–ï¼Œå·²æ›´æ–°: {current_url_parent_id[:8]}... (session: {session_id[:8]}...)")
            return current_url_parent_id
        else:
            # URLä¸­çš„parentIdä¸å­˜å‚¨çš„ç›¸åŒ
            server_logger.info(f"ğŸ”‘ ä½¿ç”¨ä¼šè¯å­˜å‚¨çš„parentId: {stored_parent_id[:8]}... (session: {session_id[:8]}...)")
            return stored_parent_id
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å­˜å‚¨çš„parentIdï¼ˆæ²¡æœ‰URLå‚æ•°çš„æƒ…å†µï¼‰
    if session_id in SESSION_PARENT_IDS:
        parent_id = SESSION_PARENT_IDS[session_id]
        server_logger.info(f"ğŸ”‘ ä½¿ç”¨ä¼šè¯å­˜å‚¨çš„parentId: {parent_id[:8]}... (session: {session_id[:8]}...)")
        return parent_id
    
    # ä½¿ç”¨é»˜è®¤é…ç½®
    default_parent_id = config.default_parent_id
    SESSION_PARENT_IDS[session_id] = default_parent_id
    server_logger.info(f"ğŸ”‘ ä½¿ç”¨é»˜è®¤parentId: {default_parent_id[:8]}... (session: {session_id[:8]}...)")
    return default_parent_id

# åˆ›å»ºFastMCPå®ä¾‹
mcp = FastMCP("çŸ¥è¯†åº“ç®¡ç†å·¥å…· v2.0")

# åˆ›å»ºæœåŠ¡å®ä¾‹
tree_service = TreeService()
search_service = SearchService()
collection_service = CollectionService()



@mcp.tool("get_dataset_tree")
async def get_kb_tree(search_value: str = "", deep: int = 4, ctx: Context = None) -> str:
    """
    è·å–çŸ¥è¯†åº“ç›®å½•æ ‘
    
    æµè§ˆçŸ¥è¯†åº“çš„ç›®å½•ç»“æ„ï¼ŒæŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†å’Œæ–‡ä»¶å¤¹ã€‚
    ç”¨äºäº†è§£çŸ¥è¯†åº“çš„ç»„ç»‡æ¶æ„ï¼Œæ‰¾åˆ°ç›¸å…³çš„æ•°æ®é›†IDç”¨äºåç»­æœç´¢ã€‚
    
    Args:
        search_value: è¿‡æ»¤å…³é”®è¯ï¼ˆå¯é€‰ï¼‰ï¼Œæ”¯æŒå¤šå…³é”®è¯ç©ºæ ¼åˆ†éš”ï¼Œå¦‚"** IPOSS"æˆ–"ç½‘ç»œç®¡ç† ç³»ç»Ÿ"
        deep: ç›®å½•æ·±åº¦ï¼ˆ1-10ï¼Œé»˜è®¤4ï¼‰
    
    Returns:
        åŒ…å«æ•°æ®é›†IDã€åç§°ã€ç±»å‹çš„ç›®å½•æ ‘ç»“æ„
    """
    parent_id = get_parent_id_from_context(ctx)
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    from src.logger import server_logger
    server_logger.info(f"ğŸ” å½“å‰ä½¿ç”¨çš„parentId: {parent_id}")
    
    return await tree_service.get_knowledge_base_tree(parent_id, search_value, deep)

@mcp.tool("search_dataset")
async def search_kb(dataset_id: str, text: str, limit: int = 10, ctx: Context = None) -> str:
    """
    å•æ•°æ®é›†ç²¾ç¡®æœç´¢
    
    åœ¨æŒ‡å®šçš„å•ä¸ªæ•°æ®é›†ä¸­æœç´¢ç›¸å…³å†…å®¹ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚
    é€‚ç”¨äºå·²çŸ¥ç›®æ ‡æ•°æ®é›†çš„ç²¾ç¡®æœç´¢ï¼Œæ‰¾åˆ°ç‰¹å®šæ–‡æ¡£çš„ç›¸å…³ç‰‡æ®µã€‚
    
    Args:
        dataset_id: æ•°æ®é›†IDï¼ˆé€šè¿‡get_dataset_treeè·å–ï¼‰
        text: æœç´¢å…³é”®è¯
        limit: ç»“æœæ•°é‡ï¼ˆ1-50ï¼Œé»˜è®¤10ï¼‰
    
    Returns:
        åŒ…å«æ–‡æ¡£ç‰‡æ®µã€ç›¸å…³æ€§è¯„åˆ†ã€collectionIdã€æ–‡ä»¶åå’Œä¸‹è½½é“¾æ¥çš„æœç´¢ç»“æœ
    """
    return await search_service.search_knowledge_base(dataset_id, text, limit)

@mcp.tool("view_collection_content")
async def view_collection_content_tool(collection_id: str, page_size: int = 50, ctx: Context = None) -> str:
    """
    æŸ¥çœ‹æ–‡æ¡£å®Œæ•´å†…å®¹
    
    è·å–æŒ‡å®šæ–‡æ¡£ï¼ˆcollectionï¼‰çš„æ‰€æœ‰å†…å®¹å—ï¼ŒæŸ¥çœ‹å®Œæ•´çš„æ–‡æ¡£å†…å®¹ã€‚
    é€‚ç”¨äºæŸ¥çœ‹æœç´¢åˆ°çš„æ–‡æ¡£çš„å®Œæ•´ä¿¡æ¯ã€‚
    
    Args:
        collection_id: æ–‡æ¡£IDï¼ˆä»æœç´¢ç»“æœä¸­è·å–ï¼‰
        page_size: æ¯é¡µæ•°æ®å—æ•°é‡ï¼ˆ10-100ï¼Œé»˜è®¤50ï¼‰
    
    Returns:
        åŒ…å«å®Œæ•´æ–‡æ¡£å†…å®¹ã€æ–‡ä»¶ä¿¡æ¯å’Œä¸‹è½½é“¾æ¥çš„è¯¦ç»†æŠ¥å‘Š
    """
    return await collection_service.view_collection_content(collection_id, page_size)

@mcp.tool("multi_dataset_search")
async def multi_dataset_search(dataset_ids: List[str], query: str, limit_per_dataset: int = 5, ctx: Context = None) -> str:
    """
    å¤šæ•°æ®é›†å¿«é€Ÿæœç´¢
    
    åœ¨å¤šä¸ªæ•°æ®é›†ä¸­å¹¶è¡Œæœç´¢ï¼Œå¿«é€Ÿè·å–ç›¸å…³ä¿¡æ¯æ¦‚è§ˆã€‚
    é€‚ç”¨äºè·¨æ•°æ®é›†çš„ä¿¡æ¯æ”¶é›†å’Œæ¯”è¾ƒåˆ†æã€‚
    
    Args:
        dataset_ids: æ•°æ®é›†IDåˆ—è¡¨
        query: æœç´¢å…³é”®è¯
        limit_per_dataset: æ¯ä¸ªæ•°æ®é›†çš„ç»“æœæ•°é‡ï¼ˆ1-20ï¼Œé»˜è®¤5ï¼‰
    
    Returns:
        æŒ‰æ•°æ®é›†åˆ†ç»„çš„æœç´¢ç»“æœæ±‡æ€»ï¼ŒåŒ…å«collectionIdã€æ–‡ä»¶åå’Œä¸‹è½½é“¾æ¥
    """
    from src.logger import server_logger
    
    if not dataset_ids:
        return "âŒ è¯·æä¾›è‡³å°‘ä¸€ä¸ªæ•°æ®é›†ID"
    
    if not query.strip():
        return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯"
    
    server_logger.info(f"å¼€å§‹å¤šæ•°æ®é›†æœç´¢ | æ•°æ®é›†æ•°é‡: {len(dataset_ids)} | å…³é”®è¯: '{query}' | æ¯ä¸ªæ•°æ®é›†é™åˆ¶: {limit_per_dataset}")
    
    # å¹¶è¡Œæœç´¢å¤šä¸ªæ•°æ®é›†
    async def search_single_dataset(dataset_id: str) -> tuple[str, str]:
        try:
            result = await search_service.search_knowledge_base(dataset_id, query, limit_per_dataset)
            return dataset_id, result
        except Exception as e:
            server_logger.error(f"æœç´¢æ•°æ®é›† {dataset_id} å¤±è´¥: {e}")
            return dataset_id, f"âŒ æœç´¢å¤±è´¥: {str(e)}"
    
    # æ‰§è¡Œå¹¶è¡Œæœç´¢
    tasks = [search_single_dataset(dataset_id) for dataset_id in dataset_ids]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # æ•´ç†ç»“æœ
    formatted_results = []
    successful_searches = 0
    
    for result in results:
        if isinstance(result, Exception):
            formatted_results.append(f"âŒ æœç´¢å¼‚å¸¸: {str(result)}")
        else:
            dataset_id, search_result = result
            if "âŒ" not in search_result:
                successful_searches += 1
            
            formatted_results.append(f"\nğŸ“ æ•°æ®é›†: {dataset_id[:8]}...\n{search_result}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary = f"""
ğŸ” å¤šæ•°æ®é›†æœç´¢å®Œæˆ

ğŸ“Š æœç´¢ç»Ÿè®¡:
â€¢ ç›®æ ‡æ•°æ®é›†: {len(dataset_ids)} ä¸ª
â€¢ æˆåŠŸæœç´¢: {successful_searches} ä¸ª
â€¢ æœç´¢å…³é”®è¯: "{query}"
â€¢ æ¯æ•°æ®é›†é™åˆ¶: {limit_per_dataset} æ¡

ğŸ“‹ æœç´¢ç»“æœ:
{''.join(formatted_results)}

ğŸ’¡ æç¤º: å¦‚éœ€æŸ¥çœ‹å®Œæ•´æ–‡æ¡£å†…å®¹ï¼Œè¯·ä½¿ç”¨ view_collection_content å·¥å…·
"""
    
    server_logger.info(f"å¤šæ•°æ®é›†æœç´¢å®Œæˆ | æˆåŠŸ: {successful_searches}/{len(dataset_ids)}")
    
    return summary

def main():
    """ä¸»å‡½æ•°"""
    from src.logger import server_logger
    
    server_logger.info("ğŸš€ å¯åŠ¨çŸ¥è¯†åº“ç®¡ç†MCPæœåŠ¡å™¨ v2.0")
    server_logger.info(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # æ˜¾ç¤ºå·¥å…·ä¿¡æ¯
    server_logger.info("ğŸ› ï¸  å·²æ³¨å†Œçš„MCPå·¥å…·:")
    server_logger.info("  ğŸ“ get_dataset_tree - è·å–çŸ¥è¯†åº“ç›®å½•æ ‘")
    server_logger.info("  ğŸ” search_dataset - å•æ•°æ®é›†ç²¾ç¡®æœç´¢")
    server_logger.info("  ğŸ“„ view_collection_content - æŸ¥çœ‹æ–‡æ¡£å®Œæ•´å†…å®¹")
    server_logger.info("  ğŸ” multi_dataset_search - å¤šæ•°æ®é›†å¿«é€Ÿæœç´¢")
    
    # å¯åŠ¨ä¿¡æ¯
    server_logger.info("ğŸŒ å¯åŠ¨SSEæœåŠ¡å™¨: http://0.0.0.0:18007")
    server_logger.info("ğŸ”— SSEç«¯ç‚¹: http://0.0.0.0:18007/sse")
    server_logger.info("âš™ï¸  MCPå®¢æˆ·ç«¯é…ç½®:")
    server_logger.info('  "url": "http://0.0.0.0:18007/sse"')
    server_logger.info("ğŸ’¡ æç¤º: URLå‚æ•°parentIdä¼šè‡ªåŠ¨æ£€æµ‹å˜åŒ–å¹¶æ›´æ–°ä¼šè¯å­˜å‚¨")
    server_logger.info("=" * 60)
    
    # ä½¿ç”¨FastMCPåŸç”ŸSSEæ”¯æŒ
    mcp.run(transport="sse", host="0.0.0.0", port=18007)

if __name__ == "__main__":
    main() 