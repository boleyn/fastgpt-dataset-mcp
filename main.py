#!/usr/bin/env python3
"""
çŸ¥è¯†åº“ç®¡ç†MCPæœåŠ¡å™¨

åŸºäºFastMCPæ„å»ºçš„çŸ¥è¯†åº“ç®¡ç†å·¥å…·ï¼Œæ”¯æŒç›®å½•æ ‘æŸ¥çœ‹ã€å†…å®¹æœç´¢å’ŒCollectionæŸ¥çœ‹åŠŸèƒ½ã€‚
é‡æ„ç‰ˆæœ¬ï¼Œé‡‡ç”¨æ›´å¥½çš„æ¶æ„è®¾è®¡å’Œç»Ÿä¸€çš„APIå®¢æˆ·ç«¯ã€‚
"""

import os
import asyncio
from typing import List, Union, Annotated
from fastmcp import FastMCP, Context
from pydantic import Field

# å¯¼å…¥æ–°çš„æ¶æ„ç»„ä»¶
from src.config import config
from src.services import TreeService, SearchService, CollectionService, FormatUtils, KeywordService

# ä¼šè¯çº§åˆ«çš„parentIdå­˜å‚¨ - ä½¿ç”¨session_idä½œä¸ºkey
SESSION_PARENT_IDS = {}

def get_parent_id_from_context(ctx: Context) -> str:
    """ä»Contextä¸­è·å–æˆ–è®¾ç½®parentId"""
    from src.logger import server_logger
    
    # è·å–ä¼šè¯æ ‡è¯†
    session_id = getattr(ctx, 'client_id', None) or getattr(ctx, 'request_id', None) or 'default'
    
    # é¦–å…ˆå°è¯•ä»HTTPè¯·æ±‚ä¸­æå–parentIdï¼ˆSSEè¿æ¥æ—¶ï¼‰
    current_url_parent_id = None
    try:
        # ä½¿ç”¨æ–°çš„ä¾èµ–å‡½æ•°è·å–HTTPè¯·æ±‚
        from fastmcp.server.dependencies import get_http_request
        request = get_http_request()
        
        if request and hasattr(request, 'query_params'):
            query_params = request.query_params
            if 'parentId' in query_params:
                parent_id = query_params['parentId']
                if parent_id and parent_id.strip():
                    current_url_parent_id = parent_id.strip()
    except Exception as e:
        server_logger.debug(f"æ— æ³•è·å–HTTPè¯·æ±‚æˆ–æå–URLå‚æ•°: {e}")
    
    # å¦‚æœURLä¸­æœ‰parentIdï¼Œæ£€æŸ¥æ˜¯å¦ä¸ä¼šè¯ä¸­å­˜å‚¨çš„ä¸åŒ
    if current_url_parent_id:
        stored_parent_id = SESSION_PARENT_IDS.get(session_id)
        
        if stored_parent_id != current_url_parent_id:
            # URLä¸­çš„parentIdä¸å­˜å‚¨çš„ä¸åŒï¼Œæ›´æ–°å­˜å‚¨
            SESSION_PARENT_IDS[session_id] = current_url_parent_id
            server_logger.info(f"ğŸ”„ æ£€æµ‹åˆ°parentIdå˜åŒ–ï¼Œå·²æ›´æ–°: {current_url_parent_id[:8]}... (session: {session_id[:8]}...)")
            return current_url_parent_id
        else:
            # URLä¸­çš„parentIdä¸å­˜å‚¨çš„ç›¸åŒ
            server_logger.info(f"ğŸ”‘ ä½¿ç”¨ä¼šè¯å­˜å‚¨çš„parentId: {stored_parent_id[:8]}... (session: {session_id[:8]}...)")
            return stored_parent_id
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å­˜å‚¨çš„parentIdï¼ˆæ²¡æœ‰URLå‚æ•°çš„æƒ…å†µï¼‰
    if session_id in SESSION_PARENT_IDS:
        parent_id = SESSION_PARENT_IDS[session_id]
        server_logger.info(f"ğŸ”‘ ä½¿ç”¨ä¼šè¯å­˜å‚¨çš„parentId: {parent_id[:8]}... (session: {session_id[:8]}...)")
        return parent_id
    
    # ä½¿ç”¨é»˜è®¤é…ç½®
    default_parent_id = config.default_parent_id
    SESSION_PARENT_IDS[session_id] = default_parent_id
    server_logger.info(f"ğŸ”‘ ä½¿ç”¨é»˜è®¤parentId: {default_parent_id[:8]}... (session: {session_id[:8]}...)")
    return default_parent_id

# åˆ›å»ºFastMCPå®ä¾‹
mcp = FastMCP("çŸ¥è¯†åº“ç®¡ç†å·¥å…· v2.0")

# åˆ›å»ºæœåŠ¡å®ä¾‹
tree_service = TreeService()
search_service = SearchService()
collection_service = CollectionService()
keyword_service = KeywordService()



@mcp.tool()
async def get_dataset_tree(
    search_value: Annotated[str, Field(description="è¿‡æ»¤å…³é”®è¯ï¼ˆå¯é€‰ï¼‰ï¼Œæ”¯æŒå¤šå…³é”®è¯ç©ºæ ¼åˆ†éš”ï¼Œå¦‚'** IPOSS'æˆ–'ç½‘ç»œç®¡ç† ç³»ç»Ÿ'")] = "",
    deep: Annotated[int, Field(description="ç›®å½•æ·±åº¦ï¼ˆ1-10ï¼Œé»˜è®¤4ï¼‰", ge=1, le=10)] = 4,
    ctx: Context = None
) -> str:
    """
    è·å–çŸ¥è¯†åº“ç›®å½•æ ‘
    
    æµè§ˆçŸ¥è¯†åº“çš„ç›®å½•ç»“æ„ï¼ŒæŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†å’Œæ–‡ä»¶å¤¹ã€‚
    ç”¨äºäº†è§£çŸ¥è¯†åº“çš„ç»„ç»‡æ¶æ„ï¼Œæ‰¾åˆ°ç›¸å…³çš„æ•°æ®é›†IDç”¨äºåç»­æœç´¢ã€‚
    """
    parent_id = get_parent_id_from_context(ctx)
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    from src.logger import server_logger
    server_logger.info(f"ğŸ” å½“å‰ä½¿ç”¨çš„parentId: {parent_id}")
    
    return await tree_service.get_knowledge_base_tree(parent_id, search_value, deep)

@mcp.tool()
async def search_dataset(
    dataset_id: Annotated[str, Field(description="æ•°æ®é›†IDï¼ˆé€šè¿‡get_dataset_treeè·å–ï¼‰")],
    text: Annotated[str, Field(description="æœç´¢å…³é”®è¯")],
    limit: Annotated[int, Field(description="ç»“æœæ•°é‡ï¼ˆ1-50ï¼Œé»˜è®¤10ï¼‰", ge=1, le=50)] = 10,
    ctx: Context = None
) -> str:
    """
    å•æ•°æ®é›†ç²¾ç¡®æœç´¢
    
    åœ¨æŒ‡å®šçš„å•ä¸ªæ•°æ®é›†ä¸­æœç´¢ç›¸å…³å†…å®¹ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚
    é€‚ç”¨äºå·²çŸ¥ç›®æ ‡æ•°æ®é›†çš„ç²¾ç¡®æœç´¢ï¼Œæ‰¾åˆ°ç‰¹å®šæ–‡æ¡£çš„ç›¸å…³ç‰‡æ®µã€‚
    """
    return await search_service.search_knowledge_base(dataset_id, text, limit)

@mcp.tool()
async def view_collection_content(
    collection_id: Annotated[str, Field(description="æ–‡æ¡£IDï¼ˆä»æœç´¢ç»“æœä¸­è·å–ï¼‰")],
    page_size: Annotated[int, Field(description="æ¯é¡µæ•°æ®å—æ•°é‡ï¼ˆ10-100ï¼Œé»˜è®¤50ï¼‰", ge=10, le=100)] = 50,
    ctx: Context = None
) -> str:
    """
    æŸ¥çœ‹æ–‡æ¡£å®Œæ•´å†…å®¹
    
    è·å–æŒ‡å®šæ–‡æ¡£ï¼ˆcollectionï¼‰çš„æ‰€æœ‰å†…å®¹å—ï¼ŒæŸ¥çœ‹å®Œæ•´çš„æ–‡æ¡£å†…å®¹ã€‚
    é€‚ç”¨äºæŸ¥çœ‹æœç´¢åˆ°çš„æ–‡æ¡£çš„å®Œæ•´ä¿¡æ¯ã€‚
    """
    return await collection_service.view_collection_content(collection_id, page_size)

@mcp.tool()
async def multi_dataset_search(
    dataset_ids: Annotated[str, Field(description="æ•°æ®é›†IDçš„é€—å·åˆ†éš”å­—ç¬¦ä¸²ï¼Œæœ€å¤š5ä¸ªï¼ˆå¦‚ï¼š'id1,id2,id3'ï¼‰")],
    query: Annotated[str, Field(description="æœç´¢å…³é”®è¯")],
    limit_per_dataset: Annotated[int, Field(description="æ¯ä¸ªæ•°æ®é›†çš„ç»“æœæ•°é‡ï¼ˆ1-20ï¼Œé»˜è®¤5ï¼‰", ge=1, le=10)] = 5,
    ctx: Context = None
) -> str:
    """
    å¤šæ•°æ®é›†å¿«é€Ÿæœç´¢
    
    åœ¨å¤šä¸ªæ•°æ®é›†ä¸­å¹¶è¡Œæœç´¢ï¼Œå¿«é€Ÿè·å–ç›¸å…³ä¿¡æ¯æ¦‚è§ˆã€‚
    é€‚ç”¨äºè·¨æ•°æ®é›†çš„ä¿¡æ¯æ”¶é›†å’Œæ¯”è¾ƒåˆ†æã€‚
    """
    from src.logger import server_logger
    
    # è®°å½•åŸå§‹è¾“å…¥å‚æ•°
    server_logger.info(f"åŸå§‹dataset_idså‚æ•°: {dataset_ids} (ç±»å‹: {type(dataset_ids)})")
    
    # å¤„ç†é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
    if not isinstance(dataset_ids, str):
        server_logger.error(f"âŒ dataset_idså¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(dataset_ids)}")
        return f"âŒ dataset_idså‚æ•°å¿…é¡»æ˜¯å­—ç¬¦ä¸²"
    
    # æŒ‰é€—å·åˆ†å‰²å¹¶å»é™¤ç©ºç™½
    dataset_ids_list = [id.strip() for id in dataset_ids.split(",") if id.strip()]
    server_logger.info(f"æ£€æµ‹åˆ°é€—å·åˆ†éš”çš„dataset_idsï¼Œå·²åˆ†å‰²ä¸º {len(dataset_ids_list)} ä¸ªID")
    
    # éªŒè¯æ¯ä¸ªdataset_idçš„æ ¼å¼
    for i, dataset_id in enumerate(dataset_ids_list):
        if len(dataset_id) == 0:
            server_logger.error(f"âŒ dataset_ids[{i}] ä¸ºç©ºå­—ç¬¦ä¸²")
            return f"âŒ æ•°æ®é›†IDä¸èƒ½ä¸ºç©º"
        
        # æ£€æŸ¥æ˜¯å¦è¿˜åŒ…å«é€—å·
        if "," in dataset_id:
            server_logger.error(f"âŒ dataset_ids[{i}] ä»åŒ…å«é€—å·: {dataset_id}")
            return f"âŒ æ•°æ®é›†IDæ ¼å¼é”™è¯¯ï¼Œä¸èƒ½åŒ…å«é€—å·: {dataset_id}"
    
    if not dataset_ids_list:
        return "âŒ è¯·æä¾›è‡³å°‘ä¸€ä¸ªæ•°æ®é›†ID"
    
    if len(dataset_ids_list) > 5:
        return f"âŒ æ•°æ®é›†æ•°é‡è¶…å‡ºé™åˆ¶ï¼Œæœ€å¤šæ”¯æŒ5ä¸ªæ•°æ®é›†ï¼Œå½“å‰æä¾›äº†{len(dataset_ids_list)}ä¸ª"
    
    if not query.strip():
        return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯"
    
    # ç¡®ä¿limit_per_datasetæ˜¯æ•´æ•°ç±»å‹
    if isinstance(limit_per_dataset, str):
        try:
            limit_per_dataset = int(limit_per_dataset)
            server_logger.info(f"å°†limit_per_datasetä»å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°: {limit_per_dataset}")
        except ValueError:
            server_logger.error(f"âŒ limit_per_datasetä¸æ˜¯æœ‰æ•ˆçš„æ•°å­—: '{limit_per_dataset}'")
            return f"âŒ limit_per_datasetå¿…é¡»æ˜¯æ•°å­—: '{limit_per_dataset}'"
    
    # éªŒè¯limit_per_datasetèŒƒå›´
    if not isinstance(limit_per_dataset, int) or limit_per_dataset < 1 or limit_per_dataset > 20:
        server_logger.error(f"âŒ limit_per_datasetè¶…å‡ºèŒƒå›´ (1-20): {limit_per_dataset}")
        return f"âŒ limit_per_datasetå¿…é¡»åœ¨1-20ä¹‹é—´: {limit_per_dataset}"
    
    server_logger.info(f"å¼€å§‹å¤šæ•°æ®é›†æœç´¢ | æ•°æ®é›†æ•°é‡: {len(dataset_ids_list)} | å…³é”®è¯: '{query}' | æ¯ä¸ªæ•°æ®é›†é™åˆ¶: {limit_per_dataset}")
    
    # å¹¶è¡Œæœç´¢å¤šä¸ªæ•°æ®é›†
    async def search_single_dataset(dataset_id: str) -> tuple[str, str]:
        try:
            server_logger.info(f"æ­£åœ¨æœç´¢å•ä¸ªæ•°æ®é›†: '{dataset_id}' (é•¿åº¦: {len(dataset_id)})")
            
            result = await search_service.search_knowledge_base(dataset_id, query, limit_per_dataset)
            return dataset_id, result
        except Exception as e:
            server_logger.error(f"æœç´¢æ•°æ®é›† {dataset_id} å¤±è´¥: {e}")
            return dataset_id, f"âŒ æœç´¢å¤±è´¥: {str(e)}"
    
    # æ‰§è¡Œå¹¶è¡Œæœç´¢
    tasks = [search_single_dataset(dataset_id) for dataset_id in dataset_ids_list]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # æ•´ç†ç»“æœ
    formatted_results = []
    successful_searches = 0
    
    for result in results:
        if isinstance(result, Exception):
            formatted_results.append(f"âŒ æœç´¢å¼‚å¸¸: {str(result)}")
        else:
            dataset_id, search_result = result
            if "âŒ" not in search_result:
                successful_searches += 1
            
            formatted_results.append(f"\nğŸ“ æ•°æ®é›†: {dataset_id[:8]}...\n{search_result}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary = f"""
ğŸ” å¤šæ•°æ®é›†æœç´¢å®Œæˆ

ğŸ“Š æœç´¢ç»Ÿè®¡:
â€¢ ç›®æ ‡æ•°æ®é›†: {len(dataset_ids_list)} ä¸ª
â€¢ æˆåŠŸæœç´¢: {successful_searches} ä¸ª
â€¢ æœç´¢å…³é”®è¯: "{query}"
â€¢ æ¯æ•°æ®é›†é™åˆ¶: {limit_per_dataset} æ¡

ğŸ“‹ æœç´¢ç»“æœ:
{''.join(formatted_results)}

ğŸ’¡ æç¤º: å¦‚éœ€æŸ¥çœ‹å®Œæ•´æ–‡æ¡£å†…å®¹ï¼Œè¯·ä½¿ç”¨ view_collection_content å·¥å…·
"""
    
    server_logger.info(f"å¤šæ•°æ®é›†æœç´¢å®Œæˆ | æˆåŠŸ: {successful_searches}/{len(dataset_ids_list)}")
    
    return summary

@mcp.tool()
async def expand_search_keywords(
    original_query: Annotated[str, Field(description="åŸå§‹æœç´¢å…³é”®è¯")],
    expansion_type: Annotated[str, Field(description="æ‰©å±•ç±»å‹ (basic/comprehensive/contextualï¼Œé»˜è®¤comprehensive)")] = "comprehensive",
    ctx: Context = None
) -> str:
    """
    æ™ºèƒ½å…³é”®è¯æ‰©å±•å·¥å…·
    
    æ ¹æ®åŸå§‹æŸ¥è¯¢è¯ç”Ÿæˆæ‰©å±•å…³é”®è¯ï¼Œæ”¯æŒå¤šç§æ‰©å±•ç­–ç•¥ã€‚
    ç”¨äºå®ç°promptè¦æ±‚çš„"æ ¸å¿ƒè¯ â†’ åŒä¹‰è¯ â†’ ç›¸å…³è¯ â†’ ä¸Šä¸‹æ–‡è¯"æ‰©å±•ç­–ç•¥ã€‚
    """
    try:
        expanded_keywords = await keyword_service.expand_keywords(original_query, expansion_type)
        return keyword_service.format_expansion_result(original_query, expanded_keywords, expansion_type)
    except ValueError as e:
        return f"âŒ {str(e)}"
    except Exception as e:
        from src.logger import server_logger
        server_logger.error(f"å…³é”®è¯æ‰©å±•å¤±è´¥: {e}")
        return f"âŒ å…³é”®è¯æ‰©å±•å¤±è´¥: {str(e)}"



@mcp.tool()
async def explore_folder_contents(
    folder_id: Annotated[str, Field(description="æ–‡ä»¶å¤¹IDï¼ˆä»get_dataset_treeç»“æœä¸­è·å–ï¼‰")],
    search_value: Annotated[str, Field(description="æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰ï¼Œæ”¯æŒå¤šå…³é”®è¯ç©ºæ ¼åˆ†éš”")] = "",
    deep: Annotated[int, Field(description="æ¢ç´¢æ·±åº¦ï¼ˆ1-10ï¼Œé»˜è®¤6ï¼‰", ge=1, le=10)] = 6,
    ctx: Context = None
) -> str:
    """
    æ·±å…¥æ¢ç´¢æ–‡ä»¶å¤¹å†…å®¹
    
    å½“get_dataset_treeè¿”å›æ–‡ä»¶å¤¹æ—¶ï¼Œä½¿ç”¨æ­¤å·¥å…·æ·±å…¥æ¢ç´¢æ–‡ä»¶å¤¹å†…éƒ¨çš„æ‰€æœ‰çŸ¥è¯†åº“å’Œå­æ–‡ä»¶å¤¹ã€‚
    æ”¯æŒæ›´æ·±å±‚æ¬¡çš„æœç´¢ï¼Œå¸®åŠ©å‘ç°æ–‡ä»¶å¤¹æ·±å¤„çš„æ•°æ®é›†ã€‚
    
    Args:
        folder_id: æ–‡ä»¶å¤¹IDï¼ˆä»get_dataset_treeç»“æœä¸­è·å–ï¼‰
        search_value: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰ï¼Œæ”¯æŒå¤šå…³é”®è¯ç©ºæ ¼åˆ†éš”
        deep: æ¢ç´¢æ·±åº¦ï¼ˆ1-10ï¼Œé»˜è®¤6ï¼‰
    
    Returns:
        åŒ…å«æ–‡ä»¶å¤¹å†…æ‰€æœ‰çŸ¥è¯†åº“å’Œå­æ–‡ä»¶å¤¹çš„è¯¦ç»†æŠ¥å‘Šï¼Œä»¥åŠä½¿ç”¨å»ºè®®
    """
    return await tree_service.explore_folder_contents(folder_id, search_value, deep)

def main():
    """ä¸»å‡½æ•°"""
    from src.logger import server_logger
    
    server_logger.info("ğŸš€ å¯åŠ¨çŸ¥è¯†åº“ç®¡ç†MCPæœåŠ¡å™¨ v2.0")
    server_logger.info(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # æ˜¾ç¤ºå·¥å…·ä¿¡æ¯
    server_logger.info("ğŸ› ï¸  å·²æ³¨å†Œçš„MCPå·¥å…·:")
    server_logger.info("  ğŸ“ get_dataset_tree - è·å–çŸ¥è¯†åº“ç›®å½•æ ‘")
    server_logger.info("  ğŸ” search_dataset - å•æ•°æ®é›†ç²¾ç¡®æœç´¢")
    server_logger.info("  ğŸ“„ view_collection_content - æŸ¥çœ‹æ–‡æ¡£å®Œæ•´å†…å®¹")
    server_logger.info("  ğŸ” multi_dataset_search - å¤šæ•°æ®é›†å¿«é€Ÿæœç´¢")
    server_logger.info("  ğŸ¯ expand_search_keywords - æ™ºèƒ½å…³é”®è¯æ‰©å±•")
    server_logger.info("  ğŸ“‚ explore_folder_contents - æ·±å…¥æ¢ç´¢æ–‡ä»¶å¤¹å†…å®¹")
    
    # å¯åŠ¨ä¿¡æ¯
    server_logger.info("ğŸŒ å¯åŠ¨SSEæœåŠ¡å™¨: http://0.0.0.0:18007")
    server_logger.info("ğŸ”— SSEç«¯ç‚¹: http://0.0.0.0:18007/sse")
    server_logger.info("âš™ï¸  MCPå®¢æˆ·ç«¯é…ç½®:")
    server_logger.info('  "url": "http://0.0.0.0:18007/sse"')
    server_logger.info("ğŸ’¡ æç¤º: URLå‚æ•°parentIdä¼šè‡ªåŠ¨æ£€æµ‹å˜åŒ–å¹¶æ›´æ–°ä¼šè¯å­˜å‚¨")
    server_logger.info("=" * 60)
    
    # ä½¿ç”¨FastMCPåŸç”ŸSSEæ”¯æŒ
    mcp.run(transport="sse", host="0.0.0.0", port=18007)

if __name__ == "__main__":
    main() 