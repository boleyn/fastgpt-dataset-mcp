#!/usr/bin/env python3
"""
çŸ¥è¯†åº“ç®¡ç†MCPæœåŠ¡å™¨

åŸºäºFastMCPæ„å»ºçš„çŸ¥è¯†åº“ç®¡ç†å·¥å…·ï¼Œæ”¯æŒç›®å½•æ ‘æŸ¥çœ‹ã€å†…å®¹æœç´¢å’ŒCollectionæŸ¥çœ‹åŠŸèƒ½ã€‚
é‡æ„ç‰ˆæœ¬ï¼Œé‡‡ç”¨æ›´å¥½çš„æ¶æ„è®¾è®¡å’Œç»Ÿä¸€çš„APIå®¢æˆ·ç«¯ã€‚
"""

import os
import asyncio
from typing import List, Union
from fastmcp import FastMCP, Context

# å¯¼å…¥æ–°çš„æ¶æ„ç»„ä»¶
from src.config import config
from src.services import TreeService, SearchService, CollectionService, FormatUtils, KeywordService

# ä¼šè¯çº§åˆ«çš„parentIdå­˜å‚¨ - ä½¿ç”¨session_idä½œä¸ºkey
SESSION_PARENT_IDS = {}

def get_parent_id_from_context(ctx: Context) -> str:
    """ä»Contextä¸­è·å–æˆ–è®¾ç½®parentId"""
    from src.logger import server_logger
    
    # è·å–ä¼šè¯æ ‡è¯†
    session_id = getattr(ctx, 'client_id', None) or getattr(ctx, 'request_id', None) or 'default'
    
    # é¦–å…ˆå°è¯•ä»HTTPè¯·æ±‚ä¸­æå–parentIdï¼ˆSSEè¿æ¥æ—¶ï¼‰
    current_url_parent_id = None
    try:
        # ä½¿ç”¨æ–°çš„ä¾èµ–å‡½æ•°è·å–HTTPè¯·æ±‚
        from fastmcp.server.dependencies import get_http_request
        request = get_http_request()
        
        if request and hasattr(request, 'query_params'):
            query_params = request.query_params
            if 'parentId' in query_params:
                parent_id = query_params['parentId']
                if parent_id and parent_id.strip():
                    current_url_parent_id = parent_id.strip()
    except Exception as e:
        server_logger.debug(f"æ— æ³•è·å–HTTPè¯·æ±‚æˆ–æå–URLå‚æ•°: {e}")
    
    # å¦‚æœURLä¸­æœ‰parentIdï¼Œæ£€æŸ¥æ˜¯å¦ä¸ä¼šè¯ä¸­å­˜å‚¨çš„ä¸åŒ
    if current_url_parent_id:
        stored_parent_id = SESSION_PARENT_IDS.get(session_id)
        
        if stored_parent_id != current_url_parent_id:
            # URLä¸­çš„parentIdä¸å­˜å‚¨çš„ä¸åŒï¼Œæ›´æ–°å­˜å‚¨
            SESSION_PARENT_IDS[session_id] = current_url_parent_id
            server_logger.info(f"ğŸ”„ æ£€æµ‹åˆ°parentIdå˜åŒ–ï¼Œå·²æ›´æ–°: {current_url_parent_id[:8]}... (session: {session_id[:8]}...)")
            return current_url_parent_id
        else:
            # URLä¸­çš„parentIdä¸å­˜å‚¨çš„ç›¸åŒ
            server_logger.info(f"ğŸ”‘ ä½¿ç”¨ä¼šè¯å­˜å‚¨çš„parentId: {stored_parent_id[:8]}... (session: {session_id[:8]}...)")
            return stored_parent_id
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å­˜å‚¨çš„parentIdï¼ˆæ²¡æœ‰URLå‚æ•°çš„æƒ…å†µï¼‰
    if session_id in SESSION_PARENT_IDS:
        parent_id = SESSION_PARENT_IDS[session_id]
        server_logger.info(f"ğŸ”‘ ä½¿ç”¨ä¼šè¯å­˜å‚¨çš„parentId: {parent_id[:8]}... (session: {session_id[:8]}...)")
        return parent_id
    
    # ä½¿ç”¨é»˜è®¤é…ç½®
    default_parent_id = config.default_parent_id
    SESSION_PARENT_IDS[session_id] = default_parent_id
    server_logger.info(f"ğŸ”‘ ä½¿ç”¨é»˜è®¤parentId: {default_parent_id[:8]}... (session: {session_id[:8]}...)")
    return default_parent_id

# åˆ›å»ºFastMCPå®ä¾‹
mcp = FastMCP("çŸ¥è¯†åº“ç®¡ç†å·¥å…· v2.0")

# åˆ›å»ºæœåŠ¡å®ä¾‹
tree_service = TreeService()
search_service = SearchService()
collection_service = CollectionService()
keyword_service = KeywordService()



@mcp.tool("get_dataset_tree")
async def get_kb_tree(search_value: str = "", deep: int = 4, ctx: Context = None) -> str:
    """
    è·å–çŸ¥è¯†åº“ç›®å½•æ ‘
    
    æµè§ˆçŸ¥è¯†åº“çš„ç›®å½•ç»“æ„ï¼ŒæŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†å’Œæ–‡ä»¶å¤¹ã€‚
    ç”¨äºäº†è§£çŸ¥è¯†åº“çš„ç»„ç»‡æ¶æ„ï¼Œæ‰¾åˆ°ç›¸å…³çš„æ•°æ®é›†IDç”¨äºåç»­æœç´¢ã€‚
    
    Args:
        search_value: è¿‡æ»¤å…³é”®è¯ï¼ˆå¯é€‰ï¼‰ï¼Œæ”¯æŒå¤šå…³é”®è¯ç©ºæ ¼åˆ†éš”ï¼Œå¦‚"** IPOSS"æˆ–"ç½‘ç»œç®¡ç† ç³»ç»Ÿ"
        deep: ç›®å½•æ·±åº¦ï¼ˆ1-10ï¼Œé»˜è®¤4ï¼‰
    
    Returns:
        åŒ…å«æ•°æ®é›†IDã€åç§°ã€ç±»å‹çš„ç›®å½•æ ‘ç»“æ„
    """
    parent_id = get_parent_id_from_context(ctx)
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    from src.logger import server_logger
    server_logger.info(f"ğŸ” å½“å‰ä½¿ç”¨çš„parentId: {parent_id}")
    
    return await tree_service.get_knowledge_base_tree(parent_id, search_value, deep)

@mcp.tool("search_dataset")
async def search_kb(dataset_id: str, text: str, limit: int = 10, ctx: Context = None) -> str:
    """
    å•æ•°æ®é›†ç²¾ç¡®æœç´¢
    
    åœ¨æŒ‡å®šçš„å•ä¸ªæ•°æ®é›†ä¸­æœç´¢ç›¸å…³å†…å®¹ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚
    é€‚ç”¨äºå·²çŸ¥ç›®æ ‡æ•°æ®é›†çš„ç²¾ç¡®æœç´¢ï¼Œæ‰¾åˆ°ç‰¹å®šæ–‡æ¡£çš„ç›¸å…³ç‰‡æ®µã€‚
    
    Args:
        dataset_id: æ•°æ®é›†IDï¼ˆé€šè¿‡get_dataset_treeè·å–ï¼‰
        text: æœç´¢å…³é”®è¯
        limit: ç»“æœæ•°é‡ï¼ˆ1-50ï¼Œé»˜è®¤10ï¼‰
    
    Returns:
        åŒ…å«æ–‡æ¡£ç‰‡æ®µã€ç›¸å…³æ€§è¯„åˆ†ã€collectionIdã€æ–‡ä»¶åå’Œä¸‹è½½é“¾æ¥çš„æœç´¢ç»“æœ
    """
    return await search_service.search_knowledge_base(dataset_id, text, limit)

@mcp.tool("view_collection_content")
async def view_collection_content_tool(collection_id: str, page_size: int = 50, ctx: Context = None) -> str:
    """
    æŸ¥çœ‹æ–‡æ¡£å®Œæ•´å†…å®¹
    
    è·å–æŒ‡å®šæ–‡æ¡£ï¼ˆcollectionï¼‰çš„æ‰€æœ‰å†…å®¹å—ï¼ŒæŸ¥çœ‹å®Œæ•´çš„æ–‡æ¡£å†…å®¹ã€‚
    é€‚ç”¨äºæŸ¥çœ‹æœç´¢åˆ°çš„æ–‡æ¡£çš„å®Œæ•´ä¿¡æ¯ã€‚
    
    Args:
        collection_id: æ–‡æ¡£IDï¼ˆä»æœç´¢ç»“æœä¸­è·å–ï¼‰
        page_size: æ¯é¡µæ•°æ®å—æ•°é‡ï¼ˆ10-100ï¼Œé»˜è®¤50ï¼‰
    
    Returns:
        åŒ…å«å®Œæ•´æ–‡æ¡£å†…å®¹ã€æ–‡ä»¶ä¿¡æ¯å’Œä¸‹è½½é“¾æ¥çš„è¯¦ç»†æŠ¥å‘Š
    """
    return await collection_service.view_collection_content(collection_id, page_size)

@mcp.tool("multi_dataset_search")
async def multi_dataset_search(dataset_ids: Union[List[str], str], query: str, limit_per_dataset: int = 5, ctx: Context = None) -> str:
    """
    å¤šæ•°æ®é›†å¿«é€Ÿæœç´¢
    
    åœ¨å¤šä¸ªæ•°æ®é›†ä¸­å¹¶è¡Œæœç´¢ï¼Œå¿«é€Ÿè·å–ç›¸å…³ä¿¡æ¯æ¦‚è§ˆã€‚
    é€‚ç”¨äºè·¨æ•°æ®é›†çš„ä¿¡æ¯æ”¶é›†å’Œæ¯”è¾ƒåˆ†æã€‚
    
    Args:
        dataset_ids: æ•°æ®é›†IDåˆ—è¡¨æˆ–é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
        query: æœç´¢å…³é”®è¯
        limit_per_dataset: æ¯ä¸ªæ•°æ®é›†çš„ç»“æœæ•°é‡ï¼ˆ1-20ï¼Œé»˜è®¤5ï¼‰
    
    Returns:
        æŒ‰æ•°æ®é›†åˆ†ç»„çš„æœç´¢ç»“æœæ±‡æ€»ï¼ŒåŒ…å«collectionIdã€æ–‡ä»¶åå’Œä¸‹è½½é“¾æ¥
    """
    from src.logger import server_logger
    
    # è®°å½•åŸå§‹è¾“å…¥å‚æ•°
    server_logger.info(f"åŸå§‹dataset_idså‚æ•°: {dataset_ids} (ç±»å‹: {type(dataset_ids)})")
    
    # å…¼å®¹å­—ç¬¦ä¸²å’Œæ•°ç»„ä¸¤ç§æ ¼å¼
    if isinstance(dataset_ids, str):
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼ŒæŒ‰é€—å·åˆ†å‰²å¹¶å»é™¤ç©ºç™½
        dataset_ids = [id.strip() for id in dataset_ids.split(",") if id.strip()]
        server_logger.info(f"æ£€æµ‹åˆ°å­—ç¬¦ä¸²æ ¼å¼çš„dataset_idsï¼Œå·²è½¬æ¢ä¸ºåˆ—è¡¨: {dataset_ids}")
    elif isinstance(dataset_ids, list):
        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç¡®ä¿å»é™¤ç©ºç™½é¡¹
        dataset_ids = [str(id).strip() for id in dataset_ids if str(id).strip()]
        server_logger.info(f"æ£€æµ‹åˆ°åˆ—è¡¨æ ¼å¼çš„dataset_ids: {dataset_ids}")
    else:
        server_logger.error(f"âŒ ä¸æ”¯æŒçš„dataset_idsç±»å‹: {type(dataset_ids)}")
        return f"âŒ ä¸æ”¯æŒçš„dataset_idså‚æ•°ç±»å‹: {type(dataset_ids)}"
    
    # éªŒè¯æ¯ä¸ªdataset_idçš„æ ¼å¼
    for i, dataset_id in enumerate(dataset_ids):
        if not isinstance(dataset_id, str):
            server_logger.error(f"âŒ dataset_ids[{i}] ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {dataset_id} (ç±»å‹: {type(dataset_id)})")
            return f"âŒ æ•°æ®é›†IDå¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹"
        
        if "," in dataset_id:
            server_logger.error(f"âŒ dataset_ids[{i}] åŒ…å«é€—å·: '{dataset_id}'")
            return f"âŒ æ•°æ®é›†IDä¸èƒ½åŒ…å«é€—å·: '{dataset_id}'"
        
        if len(dataset_id.strip()) == 0:
            server_logger.error(f"âŒ dataset_ids[{i}] ä¸ºç©ºå­—ç¬¦ä¸²")
            return f"âŒ æ•°æ®é›†IDä¸èƒ½ä¸ºç©º"
    
    if not dataset_ids:
        return "âŒ è¯·æä¾›è‡³å°‘ä¸€ä¸ªæ•°æ®é›†ID"
    
    if not query.strip():
        return "âŒ è¯·æä¾›æœç´¢å…³é”®è¯"
    
    # ç¡®ä¿limit_per_datasetæ˜¯æ•´æ•°ç±»å‹
    if isinstance(limit_per_dataset, str):
        try:
            limit_per_dataset = int(limit_per_dataset)
            server_logger.info(f"å°†limit_per_datasetä»å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°: {limit_per_dataset}")
        except ValueError:
            server_logger.error(f"âŒ limit_per_datasetä¸æ˜¯æœ‰æ•ˆçš„æ•°å­—: '{limit_per_dataset}'")
            return f"âŒ limit_per_datasetå¿…é¡»æ˜¯æ•°å­—: '{limit_per_dataset}'"
    
    # éªŒè¯limit_per_datasetèŒƒå›´
    if not isinstance(limit_per_dataset, int) or limit_per_dataset < 1 or limit_per_dataset > 20:
        server_logger.error(f"âŒ limit_per_datasetè¶…å‡ºèŒƒå›´ (1-20): {limit_per_dataset}")
        return f"âŒ limit_per_datasetå¿…é¡»åœ¨1-20ä¹‹é—´: {limit_per_dataset}"
    
    server_logger.info(f"å¼€å§‹å¤šæ•°æ®é›†æœç´¢ | æ•°æ®é›†æ•°é‡: {len(dataset_ids)} | å…³é”®è¯: '{query}' | æ¯ä¸ªæ•°æ®é›†é™åˆ¶: {limit_per_dataset}")
    
    # å¹¶è¡Œæœç´¢å¤šä¸ªæ•°æ®é›†
    async def search_single_dataset(dataset_id: str) -> tuple[str, str]:
        try:
            server_logger.info(f"æ­£åœ¨æœç´¢å•ä¸ªæ•°æ®é›†: '{dataset_id}' (ç±»å‹: {type(dataset_id)}, é•¿åº¦: {len(dataset_id)})")
            
            # éªŒè¯dataset_idæ ¼å¼
            if "," in dataset_id:
                server_logger.error(f"âŒ æ£€æµ‹åˆ°å¼‚å¸¸çš„dataset_idåŒ…å«é€—å·: '{dataset_id}'")
                return dataset_id, f"âŒ æ•°æ®é›†IDæ ¼å¼é”™è¯¯: åŒ…å«é€—å·"
            
            result = await search_service.search_knowledge_base(dataset_id, query, limit_per_dataset)
            return dataset_id, result
        except Exception as e:
            server_logger.error(f"æœç´¢æ•°æ®é›† {dataset_id} å¤±è´¥: {e}")
            return dataset_id, f"âŒ æœç´¢å¤±è´¥: {str(e)}"
    
    # æ‰§è¡Œå¹¶è¡Œæœç´¢
    tasks = [search_single_dataset(dataset_id) for dataset_id in dataset_ids]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # æ•´ç†ç»“æœ
    formatted_results = []
    successful_searches = 0
    
    for result in results:
        if isinstance(result, Exception):
            formatted_results.append(f"âŒ æœç´¢å¼‚å¸¸: {str(result)}")
        else:
            dataset_id, search_result = result
            if "âŒ" not in search_result:
                successful_searches += 1
            
            formatted_results.append(f"\nğŸ“ æ•°æ®é›†: {dataset_id[:8]}...\n{search_result}")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary = f"""
ğŸ” å¤šæ•°æ®é›†æœç´¢å®Œæˆ

ğŸ“Š æœç´¢ç»Ÿè®¡:
â€¢ ç›®æ ‡æ•°æ®é›†: {len(dataset_ids)} ä¸ª
â€¢ æˆåŠŸæœç´¢: {successful_searches} ä¸ª
â€¢ æœç´¢å…³é”®è¯: "{query}"
â€¢ æ¯æ•°æ®é›†é™åˆ¶: {limit_per_dataset} æ¡

ğŸ“‹ æœç´¢ç»“æœ:
{''.join(formatted_results)}

ğŸ’¡ æç¤º: å¦‚éœ€æŸ¥çœ‹å®Œæ•´æ–‡æ¡£å†…å®¹ï¼Œè¯·ä½¿ç”¨ view_collection_content å·¥å…·
"""
    
    server_logger.info(f"å¤šæ•°æ®é›†æœç´¢å®Œæˆ | æˆåŠŸ: {successful_searches}/{len(dataset_ids)}")
    
    return summary

@mcp.tool("expand_search_keywords")
async def expand_search_keywords(original_query: str, expansion_type: str = "comprehensive", ctx: Context = None) -> str:
    """
    æ™ºèƒ½å…³é”®è¯æ‰©å±•å·¥å…·
    
    æ ¹æ®åŸå§‹æŸ¥è¯¢è¯ç”Ÿæˆæ‰©å±•å…³é”®è¯ï¼Œæ”¯æŒå¤šç§æ‰©å±•ç­–ç•¥ã€‚
    ç”¨äºå®ç°promptè¦æ±‚çš„"æ ¸å¿ƒè¯ â†’ åŒä¹‰è¯ â†’ ç›¸å…³è¯ â†’ ä¸Šä¸‹æ–‡è¯"æ‰©å±•ç­–ç•¥ã€‚
    
    Args:
        original_query: åŸå§‹æœç´¢å…³é”®è¯
        expansion_type: æ‰©å±•ç±»å‹ (basic/comprehensive/contextualï¼Œé»˜è®¤comprehensive)
    
    Returns:
        åŒ…å«åŸè¯ã€åŒä¹‰è¯ã€ç›¸å…³è¯ã€ä¸Šä¸‹æ–‡è¯çš„æ‰©å±•å…³é”®è¯åˆ—è¡¨
    """
    try:
        expanded_keywords = await keyword_service.expand_keywords(original_query, expansion_type)
        return keyword_service.format_expansion_result(original_query, expanded_keywords, expansion_type)
    except ValueError as e:
        return f"âŒ {str(e)}"
    except Exception as e:
        from src.logger import server_logger
        server_logger.error(f"å…³é”®è¯æ‰©å±•å¤±è´¥: {e}")
        return f"âŒ å…³é”®è¯æ‰©å±•å¤±è´¥: {str(e)}"



@mcp.tool("explore_folder_contents")
async def explore_folder_contents(folder_id: str, search_value: str = "", deep: int = 6, ctx: Context = None) -> str:
    """
    æ·±å…¥æ¢ç´¢æ–‡ä»¶å¤¹å†…å®¹
    
    å½“get_dataset_treeè¿”å›æ–‡ä»¶å¤¹æ—¶ï¼Œä½¿ç”¨æ­¤å·¥å…·æ·±å…¥æ¢ç´¢æ–‡ä»¶å¤¹å†…éƒ¨çš„æ‰€æœ‰çŸ¥è¯†åº“å’Œå­æ–‡ä»¶å¤¹ã€‚
    æ”¯æŒæ›´æ·±å±‚æ¬¡çš„æœç´¢ï¼Œå¸®åŠ©å‘ç°æ–‡ä»¶å¤¹æ·±å¤„çš„æ•°æ®é›†ã€‚
    
    Args:
        folder_id: æ–‡ä»¶å¤¹IDï¼ˆä»get_dataset_treeç»“æœä¸­è·å–ï¼‰
        search_value: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰ï¼Œæ”¯æŒå¤šå…³é”®è¯ç©ºæ ¼åˆ†éš”
        deep: æ¢ç´¢æ·±åº¦ï¼ˆ1-10ï¼Œé»˜è®¤6ï¼‰
    
    Returns:
        åŒ…å«æ–‡ä»¶å¤¹å†…æ‰€æœ‰çŸ¥è¯†åº“å’Œå­æ–‡ä»¶å¤¹çš„è¯¦ç»†æŠ¥å‘Šï¼Œä»¥åŠä½¿ç”¨å»ºè®®
    """
    return await tree_service.explore_folder_contents(folder_id, search_value, deep)

def main():
    """ä¸»å‡½æ•°"""
    from src.logger import server_logger
    
    server_logger.info("ğŸš€ å¯åŠ¨çŸ¥è¯†åº“ç®¡ç†MCPæœåŠ¡å™¨ v2.0")
    server_logger.info(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # æ˜¾ç¤ºå·¥å…·ä¿¡æ¯
    server_logger.info("ğŸ› ï¸  å·²æ³¨å†Œçš„MCPå·¥å…·:")
    server_logger.info("  ğŸ“ get_dataset_tree - è·å–çŸ¥è¯†åº“ç›®å½•æ ‘")
    server_logger.info("  ğŸ” search_dataset - å•æ•°æ®é›†ç²¾ç¡®æœç´¢")
    server_logger.info("  ğŸ“„ view_collection_content - æŸ¥çœ‹æ–‡æ¡£å®Œæ•´å†…å®¹")
    server_logger.info("  ğŸ” multi_dataset_search - å¤šæ•°æ®é›†å¿«é€Ÿæœç´¢")
    server_logger.info("  ğŸ¯ expand_search_keywords - æ™ºèƒ½å…³é”®è¯æ‰©å±•")
    server_logger.info("  ğŸ“‚ explore_folder_contents - æ·±å…¥æ¢ç´¢æ–‡ä»¶å¤¹å†…å®¹")
    
    # å¯åŠ¨ä¿¡æ¯
    server_logger.info("ğŸŒ å¯åŠ¨SSEæœåŠ¡å™¨: http://0.0.0.0:18007")
    server_logger.info("ğŸ”— SSEç«¯ç‚¹: http://0.0.0.0:18007/sse")
    server_logger.info("âš™ï¸  MCPå®¢æˆ·ç«¯é…ç½®:")
    server_logger.info('  "url": "http://0.0.0.0:18007/sse"')
    server_logger.info("ğŸ’¡ æç¤º: URLå‚æ•°parentIdä¼šè‡ªåŠ¨æ£€æµ‹å˜åŒ–å¹¶æ›´æ–°ä¼šè¯å­˜å‚¨")
    server_logger.info("=" * 60)
    
    # ä½¿ç”¨FastMCPåŸç”ŸSSEæ”¯æŒ
    mcp.run(transport="sse", host="0.0.0.0", port=18007)

if __name__ == "__main__":
    main() 